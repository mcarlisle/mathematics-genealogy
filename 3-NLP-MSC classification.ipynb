{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we'll do NLP on the titles of math dissertations.\n",
    "\n",
    "# First thing to do is load the combined corpus and thesis titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import seaborn as sns\n",
    "#import spacy\n",
    "import string\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"20190722/combined_msc_corpus.pickle\", \"rb\") as f:\n",
    "    combined_msc_corpus = pickle.load(f)\n",
    "with open(\"20190722/thesis_msc.pickle\", \"rb\") as f:\n",
    "    thesis_msc = pickle.load(f)\n",
    "#with open('count_rf_20190724.pickle', 'rb') as f:\n",
    "#    winning_pipe = pickle.load(f)\n",
    "with open('20190724/msc_to_fill_20190724.pickle', 'rb') as f:\n",
    "    msc_to_fill = pickle.load(f)\n",
    "with open('20190724/thesis_msc_titled_to_fill_20190724.pickle', 'rb') as f:\n",
    "    thesis_msc_titled_to_fill = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis_msc.head()\n",
    "thesis_msc_filled = thesis_msc[thesis_msc['msc']!=-1].copy()\n",
    "thesis_msc_unfilled = thesis_msc[thesis_msc['msc']==-1].copy()\n",
    "thesis_msc_titled = thesis_msc[thesis_msc['thesis']!=\"\"].copy()\n",
    "thesis_msc_untitled = thesis_msc[thesis_msc['thesis']==\"\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msc filled: 246182 = 138183 filled + 107999 unfilled\n",
      "msc titled: 246182 = 220190 titled +  25992 untitled\n"
     ]
    }
   ],
   "source": [
    "print(f\"msc filled: {len(thesis_msc)} = {len(thesis_msc_filled)} filled + {len(thesis_msc_unfilled)} unfilled\")\n",
    "print(f\"msc titled: {len(thesis_msc)} = {len(thesis_msc_titled)} titled +  {len(thesis_msc_untitled)} untitled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 25,992 theses that are not titled in this list.\n",
    "# While we can still use their classifications for mapping purposes,\n",
    "# we cannot use them for NLP purposes. Drop them here.\n",
    "\n",
    "# This means merely using thesis_msc_titled from here on.\n",
    "thesis_msc_titled_filled   = thesis_msc_titled[thesis_msc_titled['msc']!=-1].copy()\n",
    "thesis_msc_titled_unfilled = thesis_msc_titled[thesis_msc_titled['msc']==-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msc titled: 220190 = 132116 filled + 88074 unfilled\n"
     ]
    }
   ],
   "source": [
    "to_print1 = f\"msc titled: {len(thesis_msc_titled)} = \"\n",
    "to_print2 = f\"{len(thesis_msc_titled_filled)} filled + {len(thesis_msc_titled_unfilled)} unfilled\"\n",
    "print(to_print1 + to_print2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([58, 68, 74, 30, 3, 6, 65, 62, 8, 90, 0, 44, 54, 97, 5, 57, 52, 91, 37, 14, 35, 78, 60, 49, 47, 76, 94, 26, 20, 11, 85, 41, 83, 39, 42, 55, 53, 46, 13, 16, 81, 22, 51, 70, 34, 93, 92, 43, 18, 15, 17, 32, 86, 40, 1, 82, 45, 31, 28, 80, 33, 12, 19])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we need to prepare the vocabulary. Our MSC corpus keys are the MSC subject codes.\n",
    "combined_msc_corpus.keys()\n",
    "# We can examine each key's values to see how we should process our keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f6cb98fc324e0ebb47d75e1323a1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# If there is {} with \", see \" inside, remove the {} from that entry.\n",
    "# This will clean up the MSC data.\n",
    "cleaned_combined_msc_corpus = {}\n",
    "see_also_regex = r\"\\{(.*), see (.*)\\}\"\n",
    "see_also = re.compile(see_also_regex)\n",
    "for k, v in tqdm(combined_msc_corpus.items()):\n",
    "    cleaned_combined_msc_corpus[k] = []\n",
    "    for t in v:\n",
    "        if len(t)>=5: # just drop everything less than 5 char; not even an MSC code\n",
    "            cleaned_combined_msc_corpus[k].append(see_also.sub(\"\", t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two primary NLP questions to explore:\n",
    "    1. Can we classify the unclassified thesis titles according to MSC?\n",
    "    2. Can we discover an evolution of topics, with possible similarities across \"field\", \n",
    "        over time, based solely on thesis titles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from stackoverflow\n",
    "# https://stackoverflow.com/questions/48865150/pipeline-for-text-cleaning-processing-in-python\n",
    "# modified by MC\n",
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "from nltk.stem.wordnet import wordnet, WordNetLemmatizer\n",
    "\n",
    "default_lemmatizer = WordNetLemmatizer()\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = set(stopwords.words('english')) \n",
    "# or any other list of your choice - wrapped in set\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "# Map POS tag to first character lemmatize() accepts\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # it's a noun if it's not found\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "def remove_special_characters(text, characters=string.punctuation.replace('-', '')):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "    return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "def lemmatize_text(text, lemmatizer=default_lemmatizer):\n",
    "    tokens = tokenize_text(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in tokens])\n",
    "\n",
    "def stem_text(text, stemmer=default_stemmer):\n",
    "    tokens = tokenize_text(text)\n",
    "    return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "def remove_stopwords(text, stop_words=default_stopwords):\n",
    "    tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "    # cleaning pipeline in this function: \n",
    "    # remove extra spaces, lowercase, remove stopwords, stem_or_lem\n",
    "    \n",
    "def clean_text(text, stem_or_lem = 'stem'):\n",
    "\n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    if stem_or_lem == 'stem':\n",
    "        text = stem_text(text) # stemming\n",
    "    elif stem_or_lem == 'lem':\n",
    "        text = lemmatize_text(text) # lemmatizing\n",
    "    else: # intentionally breaking the argument so neither occurs\n",
    "        pass \n",
    "    #text.strip(' ') # strip whitespaces again?\n",
    "    \n",
    "    # remove stems and lems of optional stopwords?\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(cleaned_combined_msc_corpus.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('cleaned_combined_msc_corpus_20190729.pickle', 'wb') as f:\n",
    "#    pickle.dump(cleaned_combined_msc_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05\n",
    "# LDA - not using... this is just a note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['thesis', 'msc'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thesis_msc_titled_filled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20190729: Must add blanks with classification msc = -1 for \"no classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate 5 blank titles to classify as msc -1.\n",
    "blanks = pd.DataFrame({ 'thesis': [\"\", \" \", \"  \", \"   \", \"\\t\"], 'msc': [-1] * 5 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis_msc_titled_filled = thesis_msc_titled_filled.append(blanks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save thesis_msc_titled_filled for train-validation-test in both NLP suites\n",
    "#with open('thesis_msc_titled_filled_20190729.pickle', 'wb') as f:\n",
    "#    pickle.dump(thesis_msc_titled_filled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thesis_msc_titled_filled are for train and test.\n",
    "# thesis_msc_titled_unfilled are for actual classification.\n",
    "# msc titled: 220190 = 132116 filled + 88074 unfilled\n",
    "\n",
    "X = thesis_msc_titled_filled['thesis'].copy()\n",
    "y = thesis_msc_titled_filled['msc'].copy()\n",
    "feature_cols = ['thesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train-validation-test split for X, y with sizes train: 64741, validation: 27747, test: 39638.\n"
     ]
    }
   ],
   "source": [
    "X_tv, X_test, y_tv, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_tv, y_tv, random_state=42, test_size=0.3)\n",
    "print(f\"Created train-validation-test split for X, y with sizes \", end=\"\")\n",
    "print(f\"train: {len(X_train)}, \", end=\"\")\n",
    "print(f\"validation: {len(X_valid)}, \", end=\"\")\n",
    "print(f\"test: {len(X_test)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.fit has problems with pandas data frames.\n",
    "X_train_list = list(X_train)\n",
    "X_valid_list = list(X_valid.values)\n",
    "X_test_list  = list(X_test.values)\n",
    "\n",
    "y_train_array = np.array(y_train)\n",
    "y_valid_array = np.array(y_valid)\n",
    "y_test_array  = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, numpy.ndarray, list, numpy.ndarray, list, numpy.ndarray)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_list), type(y_train_array), type(X_valid_list), \\\n",
    "    type(y_valid_array), type(X_test_list), type(y_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cleaned_combined_msc_corpus to the training set.\n",
    "for k, v in cleaned_combined_msc_corpus.items():\n",
    "    # convert this list into a dictionary into a data frame\n",
    "    # and append it to the training dataframe.\n",
    "    X_train_list.extend(v)\n",
    "    y_train_array = np.array(list(y_train_array) + [k]*len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203580, 203580, 27747, 27747, 39638, 39638)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_list), len(y_train_array), len(X_valid_list), len(y_valid_array), len(X_test_list), len(y_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64741, 64741, 27747, 27747, 39638, 39638)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train), len(X_valid), len(y_valid), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add French and German common words to the stop words list.\n",
    "# Other than that, these topics look relatively good.\n",
    "french_stop = ['problmes', 'le', 'par', 'non', 'une', 'para', 'du', 'dans', \n",
    "               'pour', 'sur', 'les', 'en', 'la', 'des', 'et']\n",
    "german_stop = ['theorie', 'bei', 'eine', 'ein', 'im', 'auf', 'des', 'mit', \n",
    "               'fr', 'ber', 'zur', 'die', 'von', 'und', 'der', \n",
    "               'den', 'unter', 'durch', 'einer', 'eines', 'das']\n",
    "stop_words = ENGLISH_STOP_WORDS.union(french_stop)\n",
    "stop_words = stop_words.union(german_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('custom_stop_words.pickle', 'wb') as f:\n",
    "#    pickle.dump(stop_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've integrated our MSC corpus into our training titles, \n",
    "# we can run different models to explore accuracy, etc.\n",
    "results = []\n",
    "\n",
    "vectorizers = [('count', CountVectorizer(stop_words=stop_words)), \n",
    "               ('tfidf', TfidfVectorizer(stop_words=stop_words))]\n",
    "\n",
    "\n",
    "classifiers =  [('nb', MultinomialNB(verbose=10)), \n",
    "               ('rf', RandomForestClassifier(n_estimators=10, random_state=42, verbose=10)),\n",
    "               ('knn', KNeighborsClassifier(verbose=10))  #, \n",
    "               ('lr', LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=100, \n",
    "                                         random_state=42, verbose=10)), \n",
    "               ('svc', LinearSVC(max_iter=1000, random_state=42, verbose=10))\n",
    "               ]\n",
    "# TODO add some boosters\n",
    "# count + rf appears to work fantastically...\n",
    "\n",
    "for v in vectorizers:\n",
    "    for c in classifiers:\n",
    "        print(f\"({v[0]}, {c[0]}): \", end=\"\")\n",
    "        pipe = Pipeline(steps=[v, c])\n",
    "        pipe.fit(X_train_list, y_train_array)\n",
    "        labels = pipe.predict(X_valid_list)\n",
    "# https://stackoverflow.com/questions/43162506/\n",
    "#undefinedmetricwarning-f-score-is-ill-defined-and-being-set-to-0-0-in-labels-wi\n",
    "        acc = round(accuracy_score(y_valid_array, labels), 4)\n",
    "        # average='macro' vs 'weighted'\n",
    "        f1  = round(f1_score(y_valid_array, labels, average='weighted', labels=np.unique(labels)), 4)\n",
    "        pre = round(precision_score(y_valid_array, labels, average='weighted', labels=np.unique(labels)), 4)\n",
    "        rec = round(recall_score(y_valid_array, labels, average='weighted', labels=np.unique(labels)), 4)\n",
    "\n",
    "        print(f\" acc: {acc} / f1: {f1} / pre: {pre} / rec: {rec}\")\n",
    "        results.append((v[0], c[0], acc, f1, pre, rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HERE IS THE NEARLY-99% ACCURACY MODEL\n",
    "(possibly overfit random forest?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (count, rf): acc: 0.9875 / f1: 0.9875 / pre: 0.9876 / rec: 0.9875\n",
    "# That's some absurdly high accuracy. I think we have a winner.\n",
    "winning_steps = [('count', CountVectorizer(stop_words='english')),\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=42, verbose=0))]\n",
    "winning_pipe = Pipeline(steps=winning_steps)\n",
    "winning_pipe.fit(X_train_list, y_train_array)\n",
    "winning_labels = winning_pipe.predict(X_valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_acc = round(accuracy_score(y_valid_array, winning_labels), 4)\n",
    "# average='macro' vs 'weighted'\n",
    "winning_f1  = round(f1_score(y_valid_array, winning_labels, \n",
    "                             average='weighted', labels=np.unique(winning_labels)), 4)\n",
    "winning_pre = round(precision_score(y_valid_array, winning_labels, \n",
    "                                    average='weighted', labels=np.unique(winning_labels)), 4)\n",
    "winning_rec = round(recall_score(y_valid_array, winning_labels, \n",
    "                                 average='weighted', labels=np.unique(winning_labels)), 4)\n",
    "\n",
    "print(f\"VALIDATION: acc: {winning_acc} / f1: {winning_f1} / pre: {winning_pre} / rec: {winning_rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array = np.array(X_test_list)\n",
    "#X_test_array.shape\n",
    "type(X_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to run the real test data.\n",
    "real_test_labels = winning_pipe.predict(X_test_array)\n",
    "real_acc = round(accuracy_score(y_test_array, real_test_labels), 4)\n",
    "# average='macro' vs 'weighted'\n",
    "real_f1  = round(f1_score(y_test_array, real_test_labels, \n",
    "                          average='weighted', labels=np.unique(real_test_labels)), 4)\n",
    "real_pre = round(precision_score(y_test_array, real_test_labels, \n",
    "                                 average='weighted', labels=np.unique(real_test_labels)), 4)\n",
    "real_rec = round(recall_score(y_test_array, real_test_labels, \n",
    "                              average='weighted', labels=np.unique(real_test_labels)), 4)\n",
    "\n",
    "print(f\"TEST: acc: {real_acc} / f1: {real_f1} / pre: {real_pre} / rec: {real_rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('count_rf_20190724.pickle', 'wb') as f:\n",
    "with open('count_rf_20190729.pickle', 'wb') as f:\n",
    "    pickle.dump(winning_pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thesis_msc_titled_unfilled is now ready to have their MSC classified.\n",
    "len(thesis_msc_titled_unfilled)\n",
    "thesis_msc_titled_to_fill = np.array(thesis_msc_titled_unfilled['thesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis_msc_titled_to_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msc_to_fill = winning_pipe.predict(thesis_msc_titled_to_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('msc_to_fill_20190724.pickle', 'wb') as f:\n",
    "    pickle.dump(msc_to_fill, f)\n",
    "with open('thesis_msc_titled_to_fill_20190724.pickle', 'wb') as f:\n",
    "    pickle.dump(thesis_msc_titled_to_fill, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
